{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Harry Potter and the Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions and import statements\n",
    "%matplotlib notebook\n",
    "\n",
    "from glob import glob\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from ebooklib.epub import read_epub\n",
    "from ebooklib import ITEM_DOCUMENT\n",
    "\n",
    "def readEpub(fname):\n",
    "    \"\"\"Read the epub book.\"\"\"\n",
    "    book = read_epub(fname)\n",
    "    # initialise book iterator. Not all html page are chapters, and some chapters are split.\n",
    "    # Hence the data wrangling below\n",
    "    iterator = book.get_items_of_type(ITEM_DOCUMENT)\n",
    "    chapter_number = 0\n",
    "    # main loop\n",
    "    while True:\n",
    "        # go to the next html page\n",
    "        try:\n",
    "            html = next(iterator)\n",
    "        except StopIteration:\n",
    "            break\n",
    "        content = html.get_content()\n",
    "        \n",
    "        soup = BeautifulSoup(content.decode('utf-8'), 'html.parser')\n",
    "        txt = soup.body.get_text()\n",
    "        split_txt = txt.split(\"\\n\")\n",
    "        \n",
    "        # if Chapter is in 3rd element of list this is a chapter\n",
    "        # Last chapter in series is labelled Epilogue\n",
    "        if 'Chapter' in split_txt[2] or 'Epilogue' in split_txt[2]:\n",
    "            chapter_number += 1\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        # if fith element of list is empty chapter is split into two\n",
    "        if split_txt[5] == '':\n",
    "            html = next(iterator)\n",
    "            soup = BeautifulSoup(html.get_content().decode('utf-8'), 'html.parser')\n",
    "            ntxt = soup.body.get_text()\n",
    "            split_txt = ntxt.split(\"\\n\")\n",
    "            chapter_title = split_txt[0]\n",
    "            txt = txt+ntxt\n",
    "        else:\n",
    "            chapter_title = split_txt[5]\n",
    "        \n",
    "        # yield chapter as dict with chapter number, chapter title and text\n",
    "        yield dict(chapter_number=chapter_number, chapter_title=chapter_title,text=txt)\n",
    "\n",
    "def tokenize(chapters):\n",
    "    \"\"\"run nltk.word_tokenize on HP chapters, and return in concatenated pandas dataframe\"\"\"\n",
    "    hp = []\n",
    "    for chapter in chapters:\n",
    "        chapter['token'] = nltk.word_tokenize(chapter['text'])\n",
    "        df = pd.DataFrame(chapter) # create dataframe from chapter dictionary\n",
    "        del df['text'] # remove the text column, as it is large an unnecesarry\n",
    "        hp.append(df)\n",
    "\n",
    "    hp = pd.concat(hp, ignore_index=True) # concatenate list of dataframes into one large dataframe\n",
    "    \n",
    "    #convert token column to categorical to save memory and increasing computing speed for some operations\n",
    "    hp['token'] = hp['token'].astype('category')\n",
    "    \n",
    "    # check if token is a word\n",
    "    hp['alpha'] = hp['token'].apply(lambda x: x.replace('.', '').isalpha()).astype(bool)\n",
    "    \n",
    "    return hp\n",
    "\n",
    "def ends_with_punctuation(string):\n",
    "    \"\"\"Check if string ends with punctuation. Strings of len 1 not counted\"\"\"\n",
    "    if len(string) == 1:\n",
    "        return False\n",
    "    if string[-2:] in [\"''\", \"``\"]:\n",
    "        return False\n",
    "    if string[-1].isalnum():\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first task is to import all the books using the `readEpub` function.\n",
    "\n",
    "The function returns each chapter of a book as a dict containing chapter number, and title as well as the text of the chapter. In the import loop we also add the book title and book number.\n",
    "\n",
    "The result is a list with all 199 chapters in the seven Harry Potter books."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import books\n",
    "chapters = []\n",
    "for i, fname in enumerate(sorted(glob('epub/*.epub'))):\n",
    "    for d in readEpub(fname):\n",
    "        d['book_number'] = i+1\n",
    "        d['book_title'] = fname.split(' - ')[1]\n",
    "        chapters.append(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step is to run `nltk.word_tokenize` on the text. Word tokenization splits the text into words and punctuation. \n",
    "\n",
    "Note, that punctuation is separated into their own tokens, except in abbrevations (e.g. `Mr.`).\n",
    "\n",
    "After running the tokenizer I find that 8345 of the tokens end with a punctuation character, which correspond to 0.8% of all the tokens. Some of these are cases like `Mr.` and `Mrs.`, but the majority are words at the end of regular setences where the period should have been separated into its own token.\n",
    "\n",
    "After some investigation I find that the problem is due to non-ascii characters that the tokenizer struggles to handle - in particular non-ascii quotation marks seem to be a problem. Also cases where there is a space between the period and an end quotation mark is a problem (e.g. `... said Harry. ''`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-ascii characters in the text\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ord   character\n",
       "160                 1079\n",
       "172   ¬                1\n",
       "223   ß                1\n",
       "233   é               17\n",
       "249   ù                1\n",
       "750   ˮ                1\n",
       "8211  –                9\n",
       "8212  —             9694\n",
       "8216  ‘              511\n",
       "8217  ’            34978\n",
       "8220  “            37058\n",
       "8221  ”            36748\n",
       "8230  …             7957\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#non-ascii characters in HP\n",
    "non_ascii = []\n",
    "print(\"Non-ascii characters in the text\")\n",
    "for chapter in chapters:\n",
    "    for i, character in enumerate(chapter['text']):\n",
    "        if ord(character) > 127:\n",
    "            non_ascii.append(dict(ord=ord(character), character=character))\n",
    "\n",
    "non_ascii = pd.DataFrame(non_ascii)\n",
    "non_ascii.groupby(['ord', 'character']).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replacing non-ascii characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_non_ascii = {160:' ',   # chr(160) = non-breaking-space. Replace with regular space\n",
    "                     172:'',    # chr(172) = ¬. \"Not character\". Seems to be a mistake. Replace with empty string\n",
    "                     750:\"''\",  # chr(750) = ˮ. Replace with two apostrophes ('')\n",
    "                     8211:'-',  # chr(8211) = –. Replace with hyphen (-)\n",
    "                     8212:'-',  # chr(8212) = —. Replace with hyphen (-)\n",
    "                     8216:'`',  # chr(8216) = ‘. Replace with grave accent (`)\n",
    "                     8217:\"'\",  # chr(8217) = ’. Replace with apostrophe (')\n",
    "                     8220:'``', # chr(8220) = “. Replace with two grave accents (``)\n",
    "                     8221:\"''\", # chr(8221) = ”. Replace with two apostrophes ('')\n",
    "                     8230:''}   # chr(8230) = …. Replace with empty string\n",
    "\n",
    "for chapter in chapters:\n",
    "    for k, v in replace_non_ascii.items():\n",
    "        chapter['text'] = chapter['text'].replace(chr(k), v)\n",
    "    chapter['text'] = chapter['text'].replace(\". ''\", \".''\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Redoing the tokenization the number of tokens that end in a punctuation character has been greatly reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens that end with punctuation 2285. Corresponding to 0.2%\n"
     ]
    }
   ],
   "source": [
    "hp = tokenize(chapters)\n",
    "\n",
    "# index of tokens that end with punctuation (e.g. '.', ',', ':') and are longer than one character\n",
    "index = hp.token.apply(lambda x: ends_with_punctuation(x)).astype(bool)\n",
    "print(\"Number of tokens that end with punctuation {}. Corresponding to {:.1%}\".format(index.sum(), index.sum()/hp.alpha.sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much better. If search for the most common tokens that continue to end with punctuation, we see that virtually all is now acronyms and abbreviations, interspersed with a few instances where a token ends with a dash."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Mr.              1221\n",
       "Mrs.              870\n",
       "St.                64\n",
       "D.A.               11\n",
       "ill.                9\n",
       "M.                  8\n",
       "O.W.L.              8\n",
       "H.                  8\n",
       "--                  8\n",
       "Vol-                5\n",
       "T.                  5\n",
       "E.                  5\n",
       "S.P.E.W.            3\n",
       "Dr.                 3\n",
       "R.A.B.              3\n",
       "p.m.                3\n",
       "of-                 2\n",
       "N.E.W.T.            2\n",
       "L.                  2\n",
       "Messrs.             2\n",
       "S.                  2\n",
       "R.                  2\n",
       "D.                  2\n",
       "J.                  2\n",
       "him-                2\n",
       "MR.                 1\n",
       "a.m.                1\n",
       "aff-                1\n",
       "e.g.                1\n",
       "''-                 1\n",
       "                 ... \n",
       "niece               0\n",
       "niffler             0\n",
       "nifflers            0\n",
       "newts               0\n",
       "newsreader          0\n",
       "nestled             0\n",
       "nevertheless        0\n",
       "net                 0\n",
       "nettle              0\n",
       "nettled             0\n",
       "nettles             0\n",
       "network             0\n",
       "neutral             0\n",
       "nevair              0\n",
       "never               0\n",
       "never-ceasing       0\n",
       "never-ending        0\n",
       "new                 0\n",
       "newsprint           0\n",
       "newborn             0\n",
       "newcomer            0\n",
       "newcomers           0\n",
       "newfound            0\n",
       "newly               0\n",
       "news                0\n",
       "newscaster          0\n",
       "newsletter          0\n",
       "newspaper           0\n",
       "newspapers          0\n",
       "!                   0\n",
       "Name: token, Length: 26841, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hp.token[index].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we add a few additional columns to the dataframe:\n",
    "* `lower`: Same as `token` but lower-case\n",
    "* `stop`: Boolean value that indictates if the token is a stop word or not. A stop word is a word that yields little or no information about the given text (e.g. the, a, an, and, ...). A tabulated list from `nltk.stopwords` is used.\n",
    "* `capital`: Boolean value that indicates if the token starts with a capital letter. This is useful for finding candidates for Character names\n",
    "* `word_index`: index array for the words. Starts at 1 and increments up. The function interpolates over punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp['lower'] = hp['token'].apply(lambda x: x.lower()).astype('category')\n",
    "hp['stop'] = hp['lower'].apply(lambda x: x in nltk.corpus.stopwords.words('english')).astype(bool)\n",
    "hp['capital'] = hp['token'].apply(lambda x: x[0].isupper()).astype(bool)\n",
    "\n",
    "word_index = np.tile(np.nan, hp.shape[0])\n",
    "word_index[hp['alpha']] = np.arange(1, hp['alpha'].sum()+1)\n",
    "word_index = pd.Series(word_index).interpolate(method='linear').astype(int)\n",
    "\n",
    "hp['word_index'] = word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save dataframe to file for work in other notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp.to_pickle('hp.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
